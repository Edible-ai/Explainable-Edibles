{"cells": [{"cell_type": "markdown", "id": "satisfied-chess", "metadata": {}, "source": ["Time to get fancy: we're going to use our (now extremely large) dataset to perform transfer learning on a MobileNetV2-based CNN model."]}, {"cell_type": "code", "execution_count": null, "id": "million-arnold", "metadata": {}, "outputs": [], "source": ["import datetime\n", "\n", "import tensorflow as tf\n", "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array\n", "from tensorflow.keras.models import Model, load_model, Sequential\n", "from tensorflow.keras.layers import Input, Flatten, Dense, Dropout, GlobalAveragePooling2D, BatchNormalization\n", "from tensorflow.keras.applications import MobileNetV2\n", "\n"]}, {"cell_type": "markdown", "id": "magnetic-equality", "metadata": {}, "source": ["---\n", "# Ingest, Image Preprocessing and Augmentation"]}, {"cell_type": "code", "execution_count": null, "id": "useful-photography", "metadata": {}, "outputs": [], "source": ["train_data_dir = \"../../Google Drive/My Drive/498/Project/mushie_image_data/\"\n", "num_classes = 2\n", "img_width, img_height = 224, 224\n", "classes = [\"poisonous\", \"edible\"]\n", "batch_size = 32\n", "\n", "# NOTE: our model will have a single output node\n", "# This means that an output of '0' means a prediction of poisonous,\n", "# And an output of '1' means a prediction of edible\n", "# To flip this, change the order of the classes above\n"]}, {"cell_type": "markdown", "id": "psychological-final", "metadata": {}, "source": ["We'll use Keras' awesome built-in data generators to implement some image augmentation methods, as well as split the dataset into training/validation sets."]}, {"cell_type": "code", "execution_count": null, "id": "excessive-longer", "metadata": {}, "outputs": [], "source": ["train_datagen = ImageDataGenerator(\n", "    rescale=1.0 / 255,\n", "    shear_range=0.2,\n", "    zoom_range=0.2,\n", "    width_shift_range=0.2,\n", "    height_shift_range=0.2,\n", "    horizontal_flip=True,\n", "    validation_split=0.2,\n", ")\n", "\n", "print(\"Training set:\")\n", "train_generator = train_datagen.flow_from_directory(\n", "    train_data_dir,\n", "    target_size=(img_height, img_width),\n", "    batch_size=batch_size,\n", "    classes=classes,\n", "    shuffle=True,\n", "    class_mode=\"binary\",\n", "    subset=\"training\",\n", ")\n", "\n", "print(\"Validation set:\")\n", "validation_generator = train_datagen.flow_from_directory(\n", "    train_data_dir,\n", "    target_size=(img_height, img_width),\n", "    batch_size=batch_size,\n", "    classes=classes,\n", "    class_mode=\"binary\",\n", "    subset=\"validation\",\n", ")\n"]}, {"cell_type": "markdown", "id": "requested-daniel", "metadata": {}, "source": ["---\n", "# Model Setup\n", "Following the [Keras Transfer Learning guide](https://keras.io/guides/transfer_learning/). "]}, {"cell_type": "code", "execution_count": null, "id": "hybrid-straight", "metadata": {}, "outputs": [], "source": ["# Start with a MobileNetV2 base, not including the output nodes.\n", "base_model = MobileNetV2(\n", "    include_top=False, weights=\"imagenet\", input_shape=(img_width, img_height, 3)\n", ")\n"]}, {"cell_type": "markdown", "id": "capital-husband", "metadata": {}, "source": []}, {"cell_type": "code", "execution_count": null, "id": "unsigned-frederick", "metadata": {}, "outputs": [], "source": ["# We want to freeze most of the model so it retains lower-level feature extraction that it got from being trained on ImageNet.\n", "print(\"These layers will be set to untrainable: \")\n", "for layer in base_model.layers[:-11]:\n", "    layer.trainable = False\n", "    print(layer.name)\n"]}, {"cell_type": "code", "execution_count": null, "id": "friendly-dialogue", "metadata": {}, "outputs": [], "source": ["# Now we set the highest convolution block to trainable\n", "# But make sure to not allow any BatchNorm layers be trainable\n", "# https://keras.io/guides/transfer_learning/#finetuning\n", "for layer in base_model.layers[-11:]:\n", "    if \"BN\" not in layer.name and \"bn\" not in layer.name:\n", "        layer.trainable = True\n", "        print(\"\\033[93m Trainable: \", layer.name, \"\\033[0m\")\n", "    else:\n", "        layer.trainable = False\n", "        print(\"Untrainable: \", layer.name)\n"]}, {"cell_type": "code", "execution_count": null, "id": "academic-tension", "metadata": {}, "outputs": [], "source": ["# Add in new top layers\n", "# With a sigmoid output node (so we can do binary classification)\n", "x = GlobalAveragePooling2D()(base_model.output)\n", "x = Dense(64, activation=\"relu\")(x)\n", "x = Dropout(0.2)(x)\n", "x = Dense(1, activation=\"sigmoid\")(x)\n", "\n", "model = Model(inputs=base_model.input, outputs=x)\n", "model.summary()\n"]}, {"cell_type": "markdown", "id": "legendary-november", "metadata": {}, "source": ["---\n", "## Tensorboard"]}, {"cell_type": "code", "execution_count": null, "id": "upset-catholic", "metadata": {}, "outputs": [], "source": ["# To launch tensorboard, run this cell\n", "# Enable auto-reloading in the settings menu (it looks like a gear)\n", "!rm -rf ./logs/ \n", "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n", "tensorboard_cb = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n", "\n"]}, {"cell_type": "markdown", "id": "painted-print", "metadata": {}, "source": ["---\n", "# Training\n", " We're going to train for 30 epochs, using some basic metrics (accuracy, AUC) to guide us."]}, {"cell_type": "code", "execution_count": null, "id": "speaking-gardening", "metadata": {}, "outputs": [], "source": ["save_checkpoints = tf.keras.callbacks.ModelCheckpoint(\n", "    filepath=\"./tmp/checkpoint\",\n", "    save_weights_only=True,\n", "    monitor=\"val_acc\",\n", "    mode=\"max\",\n", "    save_best_only=True,\n", ")\n"]}, {"cell_type": "code", "execution_count": null, "id": "communist-count", "metadata": {}, "outputs": [], "source": ["# enable early stopping\n", "es = tf.keras.callbacks.EarlyStopping(\n", "    monitor=\"val_acc\", patience=5, mode=\"auto\", baseline=None, restore_best_weights=True\n", ")\n"]}, {"cell_type": "code", "execution_count": null, "id": "grand-glucose", "metadata": {}, "outputs": [], "source": ["# compile and train the model\n", "model.compile(\n", "    loss=\"binary_crossentropy\",\n", "    optimizer=tf.keras.optimizers.Adagrad(),\n", "    metrics=[\"acc\", \"AUC\"],\n", ")\n"]}, {"cell_type": "code", "execution_count": null, "id": "authorized-examination", "metadata": {}, "outputs": [], "source": ["EPOCHS = 30\n", "model.fit(\n", "    train_generator,\n", "    epochs=EPOCHS,\n", "    steps_per_epoch=train_generator.samples // batch_size,\n", "    validation_data=validation_generator,\n", "    validation_steps=validation_generator.samples // batch_size,\n", "    callbacks=[tensorboard_cb, save_checkpoints],\n", ")\n"]}, {"cell_type": "code", "execution_count": null, "id": "confused-decline", "metadata": {}, "outputs": [], "source": ["model.save(\"saved_model/mushie_mobilenet_partial.h5\")\n"]}, {"cell_type": "markdown", "id": "touched-radius", "metadata": {}, "source": ["---\n", "# Fine-Tuning\n", "Now we want to unfreeze the entire model to gently push up some of the features of our dataset to the lower layers.\n", "However, we don't want to destroy the low-level features from the lower layers, so we'll train at an extremely slow rate."]}, {"cell_type": "code", "execution_count": null, "id": "juvenile-container", "metadata": {}, "outputs": [], "source": ["for layer in model.layers:\n", "    # But make sure to not allow any BatchNorm layers be trainable\n", "    # https://keras.io/guides/transfer_learning/#finetuning\n", "    if \"BN\" not in layer.name and \"bn\" not in layer.name:\n", "        layer.trainable = True\n", "        print(\"Trainable: \", layer.name)\n", "    else:\n", "        layer.trainable = False\n", "        print(\"\\033[93m Untrainable: \", layer.name, \"\\033[0m\")\n"]}, {"cell_type": "code", "execution_count": null, "id": "compressed-recovery", "metadata": {}, "outputs": [], "source": ["model.summary()\n"]}, {"cell_type": "code", "execution_count": null, "id": "minimal-former", "metadata": {}, "outputs": [], "source": ["# Compile the model to learn at an extremely slow rate\n", "model.compile(\n", "    loss=\"binary_crossentropy\",\n", "    optimizer=tf.keras.optimizers.Adagrad(1e-5),\n", "    metrics=[\"acc\", \"AUC\"],\n", ")\n"]}, {"cell_type": "code", "execution_count": null, "id": "distant-teens", "metadata": {}, "outputs": [], "source": ["EPOCHS = 5\n", "model.fit(\n", "    train_generator,\n", "    epochs=EPOCHS,\n", "    steps_per_epoch=train_generator.samples // batch_size,\n", "    validation_data=validation_generator,\n", "    validation_steps=validation_generator.samples // batch_size,\n", ")\n"]}, {"cell_type": "code", "execution_count": null, "id": "advance-bobby", "metadata": {}, "outputs": [], "source": ["model.save(\"saved_model/mushie_mobilenet_finetuned.h5\")\n"]}, {"cell_type": "code", "execution_count": null, "id": "fifty-burns", "metadata": {}, "outputs": [], "source": []}], "metadata": {"celltoolbar": "Raw Cell Format", "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.7"}}, "nbformat": 4, "nbformat_minor": 5}