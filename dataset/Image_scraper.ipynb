{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "distinct-option",
   "metadata": {},
   "source": [
    "# 1: Get images and text descriptions from Mushroom.World"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flexible-malta",
   "metadata": {},
   "source": [
    "References\n",
    "* https://hackersandslackers.com/scraping-urls-with-beautifulsoup/\n",
    "* https://wodan.xyz/python-how-to-download-all-the-images-from-the-website/\n",
    "* https://github.com/Msalmannasir/Google_image_scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excellent-telephone",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "systematic-canyon",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lovely-business",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------#\n",
    "#                   SETTINGS\n",
    "#----------------------------------------------------#\n",
    "# Start URL, adjust to our needs\n",
    "url = \"http://www.mushroom.world/mushrooms/namelist\"\n",
    "tld = \"http://www.mushroom.world/\"\n",
    "save_directory = \"./mushie_image_data/\"\n",
    "\n",
    "# Spoofed HTTP headers to give us access to the page\n",
    "headers = {\n",
    "    'Access-Control-Allow-Origin': '*',\n",
    "    'Access-Control-Allow-Methods': 'GET',\n",
    "    'Access-Control-Allow-Headers': 'Content-Type',\n",
    "    'Access-Control-Max-Age': '3600',\n",
    "    'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stylish-pizza",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------#\n",
    "#                   SCRAPE_SHROOM\n",
    "#       Takes a Mushroom World mushroom URL\n",
    "#       Scrapes text data, Saves to CSV\n",
    "#       Scrapes images, saves locally\n",
    "#----------------------------------------------------#\n",
    "def scrape_shroom(url):\n",
    "    # First, download page HTML for parsing\n",
    "    req = requests.get(url, headers)\n",
    "    soup = bs(req.content, 'html.parser')\n",
    "\n",
    "    # Create the Dataframe to store all the info\n",
    "    df = pd.DataFrame(columns=['latin_name',\n",
    "                                'english_name',\n",
    "                                'edibility',\n",
    "                                'filename',\n",
    "                                'mushroomworld_url',\n",
    "                                'image_url'])\n",
    "\n",
    "    #----------- TEXT DATA WRANGLING ----------#\n",
    "    # Get the name, both english and latin\n",
    "    name = soup.find(\"div\", class_=\"caption\").get_text().lstrip()\n",
    "    try:\n",
    "        # Now split them into two vars and get rid of all the junk\n",
    "        latin_name, english_name = name.split(\"(\")\n",
    "        latin_name = latin_name.rstrip()\n",
    "        english_name = english_name.rstrip().rstrip(\")\")\n",
    "    except:\n",
    "        latin_name = name\n",
    "        english_name = \"N/A\"\n",
    "\n",
    "    # Get the edibility (it's always the fourth attribute on the information header)\n",
    "    edibility = soup.find_all(\"div\", class_=\"textus\")[3].get_text()\n",
    "    try:\n",
    "        edibility, _ = edibility.split(\" (\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    #----------- IMAGE DATA WRANGLING ----------#\n",
    "    # Extract all the \"swipebox\" elements that contain image references\n",
    "    href_list = soup.findAll(\"a\", class_=\"swipebox\")\n",
    "\n",
    "    # Now download each of these images locally\n",
    "    for image in href_list:\n",
    "        # First, concatenate the image filename with the TLD\n",
    "        # To get the image URL on Mushroom.World\n",
    "        img_url = image.get(\"href\").lstrip('/..')\n",
    "        img_url = tld + img_url\n",
    "\n",
    "        # Next, get just the image filename without any URL business\n",
    "        img_filename = image.get(\"href\").lstrip('/../data/fungi/')\n",
    "\n",
    "        # Now try to download the image from Mushroom.World\n",
    "        try:\n",
    "            response = requests.get(img_url)\n",
    "            if response.status_code == 200:\n",
    "                with open(save_directory + img_filename, 'wb') as f:\n",
    "                    f.write(requests.get(img_url).content)\n",
    "                    f.close()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Now we have to save all that image data to a data frame\n",
    "        df = df.append({ 'latin_name' : latin_name,\n",
    "                         'english_name' : english_name,\n",
    "                         'edibility' : edibility,\n",
    "                         'filename' : img_filename,\n",
    "                         'mushroomworld_url' : url,\n",
    "                         'image_url' : img_url\n",
    "            },\n",
    "            sort=False,\n",
    "            ignore_index=True\n",
    "            )\n",
    "        # END FOR\n",
    "\n",
    "    return df\n",
    "    #END SCRAPE_SHROOM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cubic-preview",
   "metadata": {},
   "source": [
    "### Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "terminal-gilbert",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------#\n",
    "#                    MAIN DRIVER\n",
    "#----------------------------------------------------#\n",
    "# Check that the file doesn't exist first before making it\n",
    "if not os.path.exists(save_directory):\n",
    "    os.makedirs(save_directory)\n",
    "\n",
    "# Create the Dataframe to store all the info\n",
    "df = pd.DataFrame(columns=['latin_name',\n",
    "                            'english_name',\n",
    "                            'edibility',\n",
    "                            'filename',\n",
    "                            'mushroomworld_url',\n",
    "                            'image_url'])\n",
    "\n",
    "# Get the URL to each mushroom page\n",
    "first_req = requests.get(url, headers)\n",
    "first_soup = bs(first_req.content, 'html.parser')\n",
    "mushroom_list = first_soup.find_all(\"a\")\n",
    "\n",
    "# For each URL, scrape the desired information using our function\n",
    "for mushroom in mushroom_list:\n",
    "    # But make sure that the URL is one we want!\n",
    "    if tld in mushroom.get(\"href\"):\n",
    "        df_out = scrape_shroom(mushroom.get(\"href\"))\n",
    "        df = df.append(df_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungry-patent",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./mushie_image_data/_scraped_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metric-batman",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace('\\n','', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amazing-sitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitted-shelter",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-network",
   "metadata": {},
   "source": [
    "---\n",
    "659 images isn't going to be nearly enough to train a computer vision model on.\n",
    "\n",
    "However, now that we have species names and edibility information, it's possible to simply search through Google Images and download more photos of each species."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atlantic-alabama",
   "metadata": {},
   "source": [
    "---\n",
    "# 2: Scrape more images from Google Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attached-programmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of the unique latin names in the dataframe, as well as each one's edibility rating\n",
    "# This can be the mapping we use to search for and add new images to the dataset\n",
    "df_uniq = df[['latin_name', 'edibility']].copy()\n",
    "df_uniq.drop_duplicates(subset='latin_name', keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifteen-hometown",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_uniq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-bailey",
   "metadata": {},
   "source": [
    "We'll use Selenium to create a Google Images scraper. If you want to run this on your device, you'll need to download the appropriate [ChromeDriver executable](http://chromedriver.chromium.org/downloads) for your OS and Chrome version, then place the executable in the \"dataset\" folder of this repo's filetree. \n",
    "\n",
    "This will allow Selenium to programmatically navigate within the Chrome browser, so you don't have to do any manual clicking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominant-commercial",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "def get_from_gimages(name, edibility):\n",
    "    # Replace any spaces in the name with + to match the google image URL query format \n",
    "    mushie_name_url = name.replace(\" \", \"+\")\n",
    "    url = 'https://www.google.com/search?q=' + mushie_name_url + '&source=lnms&tbm=isch&sa=X&ved=2ahUKEwie44_AnqLpAhUhBWMBHUFGD90Q_AUoAXoECBUQAw&biw=1920&bih=947'\n",
    "\n",
    "    # Make selenium open up a chrome page with the URL we built, give it time to load\n",
    "    driver = webdriver.Chrome('./chromedriver')\n",
    "    driver.get(url)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    #make a dataframe to pass the image metadata back out\n",
    "    df = pd.DataFrame(columns=['latin_name',\n",
    "                                'english_name',\n",
    "                                'edibility',\n",
    "                                'filename',\n",
    "                                'mushroomworld_url',\n",
    "                                'image_url'])\n",
    "    \n",
    "    #get 100 images from the search page\n",
    "    for j in range (1, 120):\n",
    "        print(\"{}: {}\".format(name, j))\n",
    "        #google puts \"related searches\" boxes every 25th element\n",
    "        #we want to avoid those because they screw up the crawl\n",
    "        if j%25 == 0:\n",
    "            continue\n",
    "        try:\n",
    "            #click on the image and get its source\n",
    "            imgurl = driver.find_element_by_xpath('//div//div//div//div//div//div//div//div//div//div['+str(j)+']//a[1]//div[1]//img[1]')\n",
    "            imgurl.click()\n",
    "            time.sleep(1)\n",
    "            img = driver.find_element_by_xpath('//body/div[2]/c-wiz/div[3]/div[2]/div[3]/div/div/div[3]/div[2]/c-wiz/div[1]/div[1]/div/div[2]/a/img').get_attribute(\"src\")\n",
    "            # try to download each image from the source\n",
    "            try:\n",
    "                response = requests.get(img, stream=True)\n",
    "                filename = '{}{}_gimages.jpg'.format(name.replace(\" \", \"\"), j)\n",
    "                image_save_path = os.path.join(save_directory, filename)\n",
    "                with open(image_save_path, 'wb') as file:\n",
    "                    shutil.copyfileobj(response.raw, file)\n",
    "\n",
    "                # Now we have to save all that image data to a data frame\n",
    "                df = df.append({ 'latin_name' : name,\n",
    "                                 'edibility' : edibility,\n",
    "                                 'filename' : filename,\n",
    "                                 'image_url' : img\n",
    "                    },\n",
    "                    sort=False,\n",
    "                    ignore_index=True\n",
    "                              )\n",
    "            except:\n",
    "                print(\"something went wrong with downloading image\")\n",
    "                pass\n",
    "        except:\n",
    "            print(\"something went wrong with clicking image\")\n",
    "    #end_for\n",
    "    return df\n",
    "#end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-stamp",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the above function for each species of shroom\n",
    "for _, row in df_uniq.iterrows():\n",
    "    new_img_df = get_from_gimages(row['latin_name'], row['edibility'])\n",
    "    \n",
    "    #then concatenate the new image data into the df\n",
    "    df = pd.concat([df, new_img_df], ignore_index=True)\n",
    "    \n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-village",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now save the data file\n",
    "df.to_csv(\"./mushie_image_data/_scraped_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abroad-blink",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
